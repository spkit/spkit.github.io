
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/information_theory/plot_it_entropy_real_valued_example.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_information_theory_plot_it_entropy_real_valued_example.py>`
        to download the full example code or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_information_theory_plot_it_entropy_real_valued_example.py:


===============================
Entropy  -  Real-Valued Source
===============================


Entropy of real-values source is computed by estimating the probability distribution of the source signal.
While doing computing it, entropy value depend a lot on number of bins used to discreetised the signal/source, so to
fairly compare two sources, either use normalised entropy or enforce the equal number of bins for both signals/sources

.. GENERATED FROM PYTHON SOURCE LINES 13-72



.. image-sg:: /auto_examples/information_theory/images/sphx_glr_plot_it_entropy_real_valued_example_001.png
   :alt: Shannan Entropy: 5.9493 bits, Shannan Entropy: 5.0568 bits
   :srcset: /auto_examples/information_theory/images/sphx_glr_plot_it_entropy_real_valued_example_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    spkit-version 0.0.9.7
    Shannan entropy
    Entropy of x: H(x) =  4.458295645225287
    Entropy of y: H(y) =  5.0568007770298

    Normalised Shannan entropy
    Entropy of x: H(x) =  0.9997452649778813
    Entropy of y: H(y) =  0.8492835159311757

    Un-normalised Shannan entropy with same number of bins
    Entropy of x: H(x) =  5.94931580038085
    Entropy of y: H(y) =  5.0568007770298

    Rényi entropy
    Entropy of x: H(x) =  5.9443311905007485
    Entropy of y: H(y) =  4.83432402148744
    /Users/nikeshbajaj/Library/CloudStorage/OneDrive-QueenMary,UniversityofLondon/Github/GIT3/Dev/SPKIT_Dev/SPKIT_DOC_7/examples/information_theory/plot_it_entropy_real_valued_example.py:61: DeprecationWarning: function HistPlot will be deprecated in future version, due to naming consistency, please use 'hist_plot' for updated/improved functionality. [spkit-0.0.9.7].
    /Users/nikeshbajaj/Library/CloudStorage/OneDrive-QueenMary,UniversityofLondon/Github/GIT3/Dev/SPKIT_Dev/SPKIT_DOC_7/examples/information_theory/plot_it_entropy_real_valued_example.py:66: DeprecationWarning: function HistPlot will be deprecated in future version, due to naming consistency, please use 'hist_plot' for updated/improved functionality. [spkit-0.0.9.7].






|

.. code-block:: Python

    import numpy as np
    import matplotlib.pyplot as plt
    import spkit as sp
    print('spkit-version',sp.__version__)

    x = np.random.rand(10000)
    y = np.random.randn(10000)

    #Shannan entropy
    H_x = sp.entropy(x,alpha=1)
    H_y = sp.entropy(y,alpha=1)
    print('Shannan entropy')
    print('Entropy of x: H(x) = ',H_x)
    print('Entropy of y: H(y) = ',H_y)
    print('')

    # Entropy of real-values source depend a lot on number of bins, so to
    # fairly compare two sources, either use normalised entropy or enforce
    # the equal number of bins for both sources

    Hn_x = sp.entropy(x,alpha=1, normalize=True)
    Hn_y = sp.entropy(y,alpha=1, normalize=True)
    print('Normalised Shannan entropy')
    print('Entropy of x: H(x) = ',Hn_x)
    print('Entropy of y: H(y) = ',Hn_y)
    print('')

    bins = int(max(sp.bin_width(x)[1], sp.bin_width(y)[1]))

    Hf_x = sp.entropy(x,alpha=1, bins=bins)
    Hf_y = sp.entropy(y,alpha=1, bins=bins)

    print('Un-normalised Shannan entropy with same number of bins')
    print('Entropy of x: H(x) = ',Hf_x)
    print('Entropy of y: H(y) = ',Hf_y)


    print('')
    #Rényi entropy
    Hr_x= sp.entropy(x,alpha=2,bins=bins)
    Hr_y= sp.entropy(y,alpha=2,bins=bins)
    print('Rényi entropy')
    print('Entropy of x: H(x) = ',Hr_x)
    print('Entropy of y: H(y) = ',Hr_y)


    plt.figure(figsize=(12,4))
    plt.subplot(121)
    sp.HistPlot(x,show=False,norm=True)
    plt.title(f'Shannan Entropy: {Hf_x.round(4)} bits')
    plt.xlabel('source: x ~ U[0-1]')
    plt.ylabel('prob.')
    plt.subplot(122)
    sp.HistPlot(y,show=False,norm=True)
    plt.title(f'Shannan Entropy: {Hf_y.round(4)} bits')
    plt.xlabel('source: y ~ N(0,1)')
    plt.ylabel('prob.')
    plt.tight_layout()
    plt.show()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.126 seconds)


.. _sphx_glr_download_auto_examples_information_theory_plot_it_entropy_real_valued_example.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/spkit/spkit/0.9.X?urlpath=lab/tree/notebooks/auto_examples/information_theory/plot_it_entropy_real_valued_example.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../../lite/lab/?path=auto_examples/information_theory/plot_it_entropy_real_valued_example.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_it_entropy_real_valued_example.ipynb <plot_it_entropy_real_valued_example.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_it_entropy_real_valued_example.py <plot_it_entropy_real_valued_example.py>`


.. include:: plot_it_entropy_real_valued_example.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
